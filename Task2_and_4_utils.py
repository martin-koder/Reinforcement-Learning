import numpy as np
import random

class E_Greedy_Policy(): # create e-greedy policy class
    
    def __init__(self, env, epsilon, decay):
        
        self.epsilon = epsilon
        self.decay = decay
        self.q_values=np.random.rand(env.N**2, 4) #initialise q-values
        
    def __call__(self, state):# Sample an action from the policy, given a state 
        
        actions = ['up','down', 'left', 'right']
        rand = np.random.random(1)
        if rand<self.epsilon:
            action = random.choice(actions) # random action determined by epsilon factor
        else:
            action_i = np.argmax(self.q_values[state]) # otherwise greedy choice of action
            action = actions[action_i]
            
        return action
        
    def update_epsilon(self): # update the epsilon factor using the decay factor
        
        self.epsilon = self.epsilon*(1-self.decay)
        
class QPolicy (E_Greedy_Policy): # qpolicy inherits from e-greedy
    
    
    def __init__(self, env, epsilon, decay, alpha, gamma):
        super().__init__(env, epsilon, decay)
        self.alpha = alpha
        self.gamma = gamma # discount factor
        
        
    def update_values(self, s_current, a_current, r_next, s_next):  #a next is idenfied differently in q learning # a current is generated by behaviour pol
        actions = ['up','down', 'left', 'right']
        a_current_i = actions.index(a_current)
        q_current = self.q_values[s_current, a_current_i]
        q_next = self.q_values[s_next].max() # no a_next is identified as with SARSA, instead q_next is identified as the max available in next state
        self.q_values[s_current, a_current_i] = q_current+self.alpha*(r_next+self.gamma*(q_next)-q_current) # this is the key update line
        

def run_qlearn_episode(env, qpolicy): #for running experiments
    s_current = env.reset() # get a starting state
    done = env.done
    r_total=0
    while not done: # until termination...
        a_current = qpolicy(s_current) # action is outputted by the qpolicy given the current state
        s_next, r_next, done = env.step(a_current) # step outputs the next state, next reward and termination
        qpolicy.update_values(s_current, a_current, r_next, s_next) # update values is called
        s_current = s_next # new state is set to current
        r_total+=r_next # rewards are aggregated
    return r_total

class SARSA (E_Greedy_Policy): # SARSA inherits from e-greedy
    
    
    def __init__(self, env, epsilon, decay, alpha, gamma):
        super().__init__(env, epsilon, decay)
        self.alpha = alpha
        self.gamma = gamma # discount factor
        
    def update_values(self, s_current, a_current, r_next, s_next, a_next): # in sarsa a next is an input, unlike with  q-learning
        actions = ['up','down', 'left', 'right']
        a_current_i = actions.index(a_current)
        a_next_i = actions.index(a_next)  #this line is extra in SARSA compared to Q-learning
        q_current = self.q_values[s_current, a_current_i]
        q_next = self.q_values[s_next,a_next_i]  #different to q-learning, here the next action is used to specify which is the next q value
        self.q_values[s_current, a_current_i] = q_current+self.alpha*(r_next+self.gamma*(q_next)-q_current) # update rule

def run_sarsa_episode(env, sarsa):#for running experiments
    s_current = env.reset()
    a_current = sarsa(s_current)  #with SARSA an action is required to begin with
    done = env.done
    r_total=0
    while not done:
        s_next, r_next, done = env.step(a_current)
        a_next = sarsa(s_next)  #a next is generated here 
        sarsa.update_values(s_current, a_current, r_next, s_next, a_next) # update values is run
        s_current = s_next
        a_current = a_next # next action must be reassigned as well as next state
        r_total+=r_next
    return r_total